{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a403e339-66ef-445b-bab0-035cd399515f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from PIL import Image\n",
    "import cv2\n",
    "\n",
    "from transformers import ResNetForImageClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3e53c30d-6d2e-4b4a-90ea-d067f9b95454",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88214b3e8d45405d9a02468ad83cffbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/69.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e2e14807f064561a8b9b69acce39e46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/102M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#load pretrained resnet model\n",
    "model = ResNetForImageClassification.from_pretrained(\"microsoft/resnet-50\")\n",
    "\n",
    "#define transforms to preprocess input image into format expected by model\n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                     std=[0.229, 0.224, 0.225])\n",
    "#inverse transform to get normalize image back to original form for visualization\n",
    "inv_normalize = transforms.Normalize(\n",
    "    mean=[-0.485/0.229, -0.456/0.224, -0.406/0.255],\n",
    "    std=[1/0.229, 1/0.224, 1/0.255]\n",
    ")\n",
    "\n",
    "#transforms to resize image to the size expected by pretrained model,\n",
    "#convert PIL image to tensor, and\n",
    "#normalize the image\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    normalize,          \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "591819aa-2e0d-47af-97bb-e5e8cdbc8ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "readImg = './eagle.jpg'\n",
    "img0 = Image.open(readImg).convert(\"RGB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "994544d9-42d4-4df1-a6b0-91027fe8eaa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "activation = {}\n",
    "def getActivation(name):\n",
    "    # the hook signature\n",
    "    def hook(module, input, output):\n",
    "        activation[name] = output.detach().cpu().numpy()\n",
    "    return hook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "23b6641e-057e-4c9e-9f0f-0cb8a16d37e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.hooks.RemovableHandle at 0x720df96fa8a0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.resnet.encoder.stages[-1].register_forward_hook(getActivation('last_stage'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b92a863a-7b39-4abd-a5f5-3f2d16f48efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = list(model.classifier[1].parameters())\n",
    "\n",
    "weight = np.squeeze(params[0].data.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ac578857-cbdd-4253-a851-7dc383483fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_CAM(feature_conv, weight, class_idx):\n",
    "    # generate the class -activation maps upsample to 256x256\n",
    "    size_upsample = (256, 256)\n",
    "    bz, nc, h, w = feature_conv.shape\n",
    "    output_cam = []\n",
    "    for idx in class_idx:\n",
    "        beforeDot =  feature_conv.reshape((nc, h*w))\n",
    "        cam = np.matmul(weight[idx], beforeDot)\n",
    "        cam = cam.reshape(h, w)\n",
    "        cam = cam - np.min(cam)\n",
    "        cam_img = cam / np.max(cam)\n",
    "        cam_img = np.uint8(255 * cam_img)\n",
    "        output_cam.append(cv2.resize(cam_img, size_upsample))\n",
    "    return output_cam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c4e8e4fd-07ad-4086-9efa-9d5dda2e50ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "logit = model(transform(img0).unsqueeze(0)).logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f60d8bce-7027-4eef-950a-0d2920bde5b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "h_x = F.softmax(logit, dim=1).data.squeeze()\n",
    "\n",
    "probs, idx = h_x.sort(0, True)\n",
    "probs = probs.detach().numpy()\n",
    "idx = idx.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8429b714-5371-4a92-8b95-d9395e2842e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "CAMs = return_CAM(activation['last_stage'], weight, [idx[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ee1c2834-c727-4a65-90c5-edb890f94a7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ WARN:0@743.390] global loadsave.cpp:848 imwrite_ Unsupported depth image for selected encoder is fallbacked to CV_8U.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = cv2.imread(readImg)\n",
    "height, width, _ = img.shape\n",
    "heatmap = cv2.applyColorMap(cv2.resize(CAMs[0],(width, height)), cv2.COLORMAP_JET)\n",
    "result = heatmap * 0.5 + img * 0.5\n",
    "\n",
    "cv2.imwrite(\"image_1.jpg\", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "55efb394-05a8-4d4d-8493-40d89de859b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bald eagle, American eagle, Haliaeetus leucocephalus'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config.id2label[idx[0]]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
