{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab3ec7f4-4bf3-4c1b-940b-7f625dc6e157",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device tensor is stored on: cuda:0\n",
      "\n",
      "<class 'list'>\n",
      "[[1, 2], [3, 4]]\n",
      "\n",
      "<class 'torch.Tensor'>\n",
      "tensor([[1, 2],\n",
      "        [3, 4]])\n",
      "\n",
      "<class 'numpy.ndarray'>\n",
      "[[1 2]\n",
      " [3 4]]\n",
      "\n",
      "<class 'torch.Tensor'>\n",
      "tensor([[1, 2],\n",
      "        [3, 4]])\n",
      "\n",
      "Ones Tensor: \n",
      " tensor([[1, 1],\n",
      "        [1, 1]]) \n",
      "\n",
      "Random Tensor: \n",
      " tensor([[0.7630, 0.0363],\n",
      "        [0.2244, 0.7537]]) \n",
      "\n",
      "Random Tensor: \n",
      " tensor([[0.9191, 0.3500, 0.8524],\n",
      "        [0.6025, 0.1849, 0.6946]]) \n",
      "\n",
      "Ones Tensor: \n",
      " tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]]) \n",
      "\n",
      "Zeros Tensor: \n",
      " tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "\n",
      "Shape of rand_tensor: \n",
      " torch.Size([2, 3])\n",
      "\n",
      "Shape of tensor: torch.Size([3, 4])\n",
      "Datatype of tensor: torch.float32\n",
      "Device tensor is stored on: cuda:0\n",
      "\n",
      "tensor([[0.5874, 0.6082, 0.0659, 0.0480],\n",
      "        [0.0012, 0.1443, 0.8151, 0.5404],\n",
      "        [0.7090, 0.5043, 0.4078, 0.3298],\n",
      "        [0.5874, 0.6082, 0.0659, 0.0480],\n",
      "        [0.0012, 0.1443, 0.8151, 0.5404],\n",
      "        [0.7090, 0.5043, 0.4078, 0.3298],\n",
      "        [0.5874, 0.6082, 0.0659, 0.0480],\n",
      "        [0.0012, 0.1443, 0.8151, 0.5404],\n",
      "        [0.7090, 0.5043, 0.4078, 0.3298]], device='cuda:0')\n",
      "\n",
      "tensor.mul(tensor) \n",
      " tensor([[3.4505e-01, 3.6995e-01, 4.3485e-03, 2.3056e-03],\n",
      "        [1.4772e-06, 2.0833e-02, 6.6444e-01, 2.9199e-01],\n",
      "        [5.0262e-01, 2.5432e-01, 1.6627e-01, 1.0880e-01]], device='cuda:0') \n",
      "\n",
      "tensor * tensor \n",
      " tensor([[3.4505e-01, 3.6995e-01, 4.3485e-03, 2.3056e-03],\n",
      "        [1.4772e-06, 2.0833e-02, 6.6444e-01, 2.9199e-01],\n",
      "        [5.0262e-01, 2.5432e-01, 1.6627e-01, 1.0880e-01]], device='cuda:0')\n",
      "\n",
      "tensor.matmul(tensor.T) \n",
      " tensor([[0.7217, 0.1682, 0.7659],\n",
      "        [0.1682, 0.9773, 0.5843],\n",
      "        [0.7659, 0.5843, 1.0320]], device='cuda:0') \n",
      "\n",
      "tensor @ tensor.T \n",
      " tensor([[0.7217, 0.1682, 0.7659],\n",
      "        [0.1682, 0.9773, 0.5843],\n",
      "        [0.7659, 0.5843, 1.0320]], device='cuda:0')\n",
      "\n",
      "tensor([[0.5874, 0.6082, 0.0659, 0.0480],\n",
      "        [0.0012, 0.1443, 0.8151, 0.5404],\n",
      "        [0.7090, 0.5043, 0.4078, 0.3298]], device='cuda:0') \n",
      "\n",
      "tensor([[5.5874, 5.6082, 5.0659, 5.0480],\n",
      "        [5.0012, 5.1443, 5.8151, 5.5404],\n",
      "        [5.7090, 5.5043, 5.4078, 5.3298]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "How to work with and operations on Tensors\n",
    "'''\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "data = [[1, 2], [3, 4]]\n",
    "np_array = np.array(data)\n",
    "x_data = torch.tensor(data)\n",
    "x_np = torch.from_numpy(np_array)\n",
    "x_ones = torch.ones_like(x_data) # retains the properties of x_data\n",
    "x_rand = torch.rand_like(x_data, dtype=torch.float) # overrides the datatype of x_data\n",
    "shape = (2, 3,)\n",
    "rand_tensor = torch.rand(shape)\n",
    "ones_tensor = torch.ones(shape)\n",
    "zeros_tensor = torch.zeros(shape)\n",
    "tensor = torch.rand(3, 4)\n",
    "if torch.cuda.is_available(): # We move our tensor to the GPU if available\n",
    "  tensor = tensor.to('cuda')\n",
    "  print(f\"Device tensor is stored on: {tensor.device}\\n\")\n",
    "t1 = torch.cat([tensor, tensor, tensor], dim=0) # 0 = vertical, 1 = horizontal\n",
    "\n",
    "\n",
    "print(type(data))\n",
    "print(data)\n",
    "print()\n",
    "print(type(x_data))\n",
    "print(x_data)\n",
    "print()\n",
    "print(type(np_array))\n",
    "print(np_array)\n",
    "print()\n",
    "print(type(x_np))\n",
    "print(x_np)\n",
    "print()\n",
    "print(f\"Ones Tensor: \\n {x_ones} \\n\")\n",
    "print(f\"Random Tensor: \\n {x_rand} \\n\")\n",
    "print(f\"Random Tensor: \\n {rand_tensor} \\n\")\n",
    "print(f\"Ones Tensor: \\n {ones_tensor} \\n\")\n",
    "print(f\"Zeros Tensor: \\n {zeros_tensor}\\n\")\n",
    "print(f\"Shape of rand_tensor: \\n {rand_tensor.shape}\\n\")\n",
    "print(f\"Shape of tensor: {tensor.shape}\")\n",
    "print(f\"Datatype of tensor: {tensor.dtype}\")\n",
    "print(f\"Device tensor is stored on: {tensor.device}\\n\")\n",
    "print(f\"{t1}\\n\")\n",
    "print(f\"tensor.mul(tensor) \\n {tensor.mul(tensor)} \\n\")#  This computes the element-wise product\n",
    "print(f\"tensor * tensor \\n {tensor * tensor}\\n\") # Alternative syntax\n",
    "print(f\"tensor.matmul(tensor.T) \\n {tensor.matmul(tensor.T)} \\n\") # matrix multiplication\n",
    "print(f\"tensor @ tensor.T \\n {tensor @ tensor.T}\\n\") # Alternative syntax\n",
    "print(tensor, \"\\n\")\n",
    "tensor.add_(5)\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5075a3af-e661-41a9-9eec-185166fb9583",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.5355e-02, 3.7194e-01, 4.8769e-01, 2.5854e-01, 1.0458e-01, 3.5576e-01,\n",
      "         8.5156e-01, 3.8040e-01, 7.2108e-01, 6.1770e-02, 4.8500e-01, 7.7654e-01,\n",
      "         1.6560e-01, 7.3190e-01, 4.6175e-01, 8.2499e-01, 9.1457e-01, 1.2807e-01,\n",
      "         3.8200e-01, 8.9577e-01, 4.0654e-02, 5.8433e-02, 4.5897e-01, 8.9397e-01,\n",
      "         8.9211e-01, 7.8768e-01, 7.3600e-01, 7.1633e-01, 8.9713e-01, 8.2204e-01,\n",
      "         4.7312e-01, 4.4901e-01, 6.0948e-01, 1.0111e-01, 6.3374e-01, 4.4760e-01,\n",
      "         9.8110e-01, 7.2244e-01, 4.9036e-01, 2.5769e-01, 2.1482e-01, 8.5950e-01,\n",
      "         7.2001e-01, 7.5535e-01, 4.1619e-01, 8.6011e-01, 3.4187e-02, 6.6285e-01,\n",
      "         6.0646e-01, 2.8766e-01, 6.5285e-01, 2.8390e-01, 7.9110e-01, 2.1669e-01,\n",
      "         9.9682e-01, 3.9596e-01, 2.9570e-02, 4.9033e-01, 7.5687e-01, 3.0433e-01,\n",
      "         2.0902e-01, 8.2820e-01, 7.6873e-02, 5.9432e-01, 8.2243e-02, 9.3149e-01,\n",
      "         2.1099e-01, 2.0438e-01, 8.5684e-01, 5.9310e-01, 6.0987e-01, 8.9900e-01,\n",
      "         8.7055e-02, 8.9929e-01, 8.7085e-01, 6.5000e-01, 8.4143e-01, 9.1813e-01,\n",
      "         5.9922e-01, 5.6411e-01, 3.4702e-01, 7.5355e-02, 9.6926e-01, 9.1808e-01,\n",
      "         9.4472e-01, 2.3939e-01, 6.6829e-01, 5.4732e-02, 2.6216e-01, 8.7699e-02,\n",
      "         4.6519e-01, 5.5490e-02, 4.7429e-01, 7.3735e-01, 9.6846e-01, 6.1728e-01,\n",
      "         7.1428e-01, 4.5593e-01, 1.9225e-01, 6.4573e-01, 1.1912e-01, 3.5636e-01,\n",
      "         3.0931e-01, 1.0522e-01, 5.7783e-01, 2.7675e-01, 2.1823e-01, 9.8540e-01,\n",
      "         7.5047e-02, 5.1124e-01, 2.8811e-02, 9.7291e-01, 5.5279e-03, 6.9837e-01,\n",
      "         2.5576e-01, 8.2445e-01, 9.2456e-01, 8.3086e-01, 8.4900e-01, 6.5365e-01,\n",
      "         8.1749e-01, 6.9603e-01, 8.5766e-01, 1.5668e-01, 7.4550e-01, 1.2335e-01,\n",
      "         2.8666e-01, 7.7010e-01, 5.8663e-01, 8.2554e-01, 2.2655e-01, 9.8075e-02,\n",
      "         5.6750e-02, 5.1392e-02, 9.1068e-01, 7.6872e-02, 9.0101e-01, 9.7092e-01,\n",
      "         9.7769e-01, 5.8336e-01, 2.0571e-01, 5.0526e-01, 5.4853e-01, 9.1906e-01,\n",
      "         9.6757e-02, 3.8825e-02, 7.9681e-01, 4.9574e-01, 7.6795e-02, 6.8957e-01,\n",
      "         5.7139e-02, 5.6567e-01, 7.9000e-01, 6.1113e-01, 1.9333e-01, 9.5429e-02,\n",
      "         3.7401e-02, 9.5784e-02, 3.2345e-01, 4.0334e-02, 8.7672e-01, 5.0748e-01,\n",
      "         7.7101e-01, 2.5310e-01, 3.2779e-01, 2.8394e-03, 2.4485e-01, 6.0665e-02,\n",
      "         6.8736e-01, 2.7161e-01, 8.7875e-01, 6.2504e-01, 3.2455e-01, 4.5431e-01,\n",
      "         8.4344e-01, 6.0009e-01, 6.6971e-01, 1.5958e-01, 5.0638e-02, 6.9205e-01,\n",
      "         9.2686e-01, 1.2169e-01, 6.5147e-01, 1.1085e-01, 3.3831e-01, 6.7640e-01,\n",
      "         7.0430e-01, 5.7123e-01, 2.1244e-01, 9.2717e-01, 5.8097e-01, 9.0493e-01,\n",
      "         5.1557e-01, 2.0432e-01, 9.3054e-01, 3.8358e-01, 1.2034e-01, 1.9525e-01,\n",
      "         9.4345e-01, 9.5762e-01, 2.1087e-01, 3.5733e-01, 9.2565e-01, 1.9029e-01,\n",
      "         1.9740e-01, 4.2502e-02, 9.5224e-01, 5.2739e-01, 4.8001e-01, 3.7419e-01,\n",
      "         4.7847e-01, 9.8330e-01, 5.0728e-01, 2.7821e-01, 1.3584e-01, 9.5291e-01,\n",
      "         6.9048e-01, 4.4139e-01, 1.2489e-01, 7.1309e-01, 1.5529e-01, 9.5838e-01,\n",
      "         6.6601e-01, 4.8113e-01, 7.6448e-01, 2.7987e-01, 8.4665e-01, 6.8672e-01,\n",
      "         8.2131e-01, 4.8921e-01, 1.6242e-01, 3.6722e-01, 2.3791e-01, 9.3111e-01,\n",
      "         9.7964e-01, 9.7588e-01, 1.2758e-01, 7.4046e-01, 4.4243e-01, 3.6296e-01,\n",
      "         2.4075e-01, 3.4170e-01, 2.5068e-01, 2.7962e-01, 3.8672e-01, 9.1100e-01,\n",
      "         9.6802e-01, 5.4477e-01, 8.7733e-01, 4.6060e-01, 1.0197e-01, 2.7210e-01,\n",
      "         1.7999e-01, 9.6317e-01, 9.0983e-01, 9.2681e-01, 1.4327e-01, 9.9381e-01,\n",
      "         6.0246e-01, 5.8943e-01, 6.9993e-01, 3.8003e-01, 3.5016e-01, 4.4486e-01,\n",
      "         4.4830e-01, 9.6152e-01, 1.1872e-01, 6.9839e-01, 3.9402e-01, 4.9733e-02,\n",
      "         3.1449e-01, 3.8092e-01, 1.5571e-01, 5.6593e-01, 6.1670e-01, 3.9175e-01,\n",
      "         2.0887e-01, 5.0450e-02, 3.6598e-01, 4.3464e-02, 5.2179e-01, 5.2315e-01,\n",
      "         1.3554e-01, 4.6276e-01, 2.7738e-01, 2.5221e-01, 1.5385e-02, 7.1703e-01,\n",
      "         5.9071e-01, 1.4847e-02, 4.7071e-01, 9.0277e-01, 5.6398e-01, 9.4703e-01,\n",
      "         2.4046e-01, 9.3087e-01, 2.9423e-03, 6.5242e-03, 7.2826e-01, 8.4807e-01,\n",
      "         4.6425e-02, 2.9835e-01, 2.1988e-01, 2.4419e-01, 2.2246e-01, 2.6297e-01,\n",
      "         4.1989e-01, 2.3824e-01, 6.6643e-01, 2.6063e-01, 7.0726e-01, 8.5463e-01,\n",
      "         7.9387e-01, 3.1415e-01, 4.2706e-01, 8.5608e-02, 8.4578e-01, 9.3446e-01,\n",
      "         6.9736e-01, 1.8443e-01, 7.2256e-01, 8.5689e-01, 9.1954e-01, 5.0941e-01,\n",
      "         1.0153e-01, 4.8050e-01, 9.7287e-01, 4.9956e-01, 6.9683e-01, 2.4778e-01,\n",
      "         1.9538e-01, 6.9877e-01, 8.9552e-01, 3.4223e-01, 6.7936e-01, 3.9723e-01,\n",
      "         2.6243e-02, 8.0046e-01, 5.9443e-01, 6.4991e-02, 8.8295e-01, 3.2921e-01,\n",
      "         5.9611e-01, 8.9320e-01, 9.4806e-01, 3.1200e-01, 1.9638e-01, 3.0540e-01,\n",
      "         9.9828e-01, 9.1463e-01, 5.8985e-01, 1.6688e-01, 4.0134e-01, 6.7973e-01,\n",
      "         4.9531e-01, 1.8417e-02, 6.0496e-01, 8.2084e-01, 8.2797e-01, 4.1807e-01,\n",
      "         4.4033e-01, 5.1848e-01, 2.6305e-01, 1.2751e-01, 1.6864e-01, 9.1878e-01,\n",
      "         4.8696e-01, 3.1667e-01, 4.6929e-01, 2.5029e-01, 8.9425e-01, 7.0651e-01,\n",
      "         1.4935e-01, 2.6287e-01, 2.0998e-01, 4.2335e-01, 8.2399e-01, 2.9167e-01,\n",
      "         4.6889e-01, 9.8433e-01, 5.7125e-01, 4.8617e-01, 9.3823e-01, 7.4157e-01,\n",
      "         7.0265e-01, 4.5737e-01, 5.7780e-02, 3.8384e-01, 9.8686e-01, 6.7497e-01,\n",
      "         6.3266e-01, 2.8315e-01, 6.3232e-01, 4.2101e-01, 2.5945e-01, 6.9228e-01,\n",
      "         4.1958e-02, 8.4139e-01, 1.9185e-01, 9.7032e-01, 5.5260e-01, 7.8861e-01,\n",
      "         5.6139e-01, 7.6981e-02, 9.4841e-01, 1.8109e-01, 1.8374e-01, 2.1131e-01,\n",
      "         6.3173e-01, 1.2576e-01, 6.8265e-01, 9.5153e-01, 2.7976e-01, 1.4674e-01,\n",
      "         5.8815e-01, 4.0193e-02, 5.5303e-01, 5.4545e-01, 6.4033e-01, 5.3939e-01,\n",
      "         4.8199e-01, 3.4453e-01, 7.4866e-01, 4.5965e-01, 2.0343e-01, 9.5866e-01,\n",
      "         2.1040e-01, 5.0399e-01, 8.4377e-01, 5.5760e-01, 8.7103e-02, 6.3144e-01,\n",
      "         1.7349e-01, 7.8559e-01, 3.3350e-01, 2.2102e-01, 9.5598e-01, 4.7628e-01,\n",
      "         9.5939e-01, 3.0768e-01, 4.3422e-01, 3.7876e-01, 5.9339e-01, 7.2632e-01,\n",
      "         3.8287e-01, 5.8231e-01, 1.8090e-01, 4.1874e-01, 6.2181e-01, 2.1479e-01,\n",
      "         3.6820e-01, 2.9646e-01, 4.6613e-01, 3.5974e-01, 6.0439e-01, 9.5431e-01,\n",
      "         1.6792e-02, 4.1375e-02, 9.2764e-01, 8.6881e-01, 9.2092e-01, 6.8931e-01,\n",
      "         4.6843e-01, 5.4687e-01, 8.5279e-01, 5.0988e-01, 2.5190e-01, 7.5397e-01,\n",
      "         8.4685e-01, 8.8294e-01, 3.2503e-01, 5.4500e-01, 2.8636e-01, 3.7344e-01,\n",
      "         4.5427e-01, 1.4035e-01, 6.8384e-01, 6.4303e-01, 1.8499e-02, 1.6861e-01,\n",
      "         8.5599e-01, 1.7931e-01, 3.5657e-01, 1.0051e-02, 2.1020e-02, 6.3077e-01,\n",
      "         6.8226e-01, 7.1497e-01, 3.6784e-01, 2.2744e-01, 6.8217e-01, 8.7972e-01,\n",
      "         7.7627e-01, 4.4615e-01, 4.4235e-01, 8.8847e-01, 9.6274e-01, 8.5277e-01,\n",
      "         3.0906e-01, 9.6345e-01, 9.2373e-01, 3.0590e-01, 1.5741e-01, 2.5621e-01,\n",
      "         4.9560e-01, 2.1070e-01, 6.3060e-02, 2.0784e-01, 9.5194e-01, 1.5212e-01,\n",
      "         3.9153e-01, 6.5827e-01, 6.3328e-01, 3.1525e-01, 2.9230e-01, 3.4547e-01,\n",
      "         2.4067e-01, 6.3738e-01, 8.5195e-01, 6.6704e-01, 8.4408e-01, 2.0976e-01,\n",
      "         7.0427e-01, 2.9323e-02, 4.9208e-01, 1.6435e-01, 2.6347e-01, 2.7185e-01,\n",
      "         4.8504e-01, 1.0600e-01, 3.0555e-01, 2.4381e-01, 2.8905e-01, 5.3819e-01,\n",
      "         5.7157e-01, 3.0441e-01, 3.5601e-01, 3.0544e-01, 8.7878e-01, 3.1090e-02,\n",
      "         7.2300e-01, 1.0707e-02, 8.8323e-01, 7.1714e-01, 3.7015e-01, 5.3574e-01,\n",
      "         7.9532e-01, 3.0150e-01, 1.3875e-01, 8.0718e-01, 5.0342e-01, 2.0052e-01,\n",
      "         5.3115e-01, 2.5848e-01, 4.4936e-01, 6.7273e-01, 9.0518e-01, 3.9785e-01,\n",
      "         9.0459e-01, 7.7344e-01, 7.3149e-02, 3.9590e-01, 6.6763e-01, 2.1711e-01,\n",
      "         4.3937e-02, 9.1343e-01, 1.9982e-01, 5.9634e-01, 5.7997e-01, 3.9855e-01,\n",
      "         3.1149e-01, 5.4038e-01, 7.6244e-01, 1.5366e-01, 9.2897e-01, 9.2204e-01,\n",
      "         2.2360e-01, 6.3692e-01, 4.7613e-01, 1.9243e-01, 5.2461e-01, 1.2545e-01,\n",
      "         4.1019e-01, 6.7123e-01, 9.8347e-02, 9.4508e-01, 5.1495e-01, 1.6793e-01,\n",
      "         9.1452e-01, 8.1053e-01, 3.7167e-01, 6.0563e-01, 5.1486e-01, 8.2430e-01,\n",
      "         5.6874e-01, 8.9993e-01, 8.3203e-02, 3.9860e-01, 4.2260e-01, 4.7991e-01,\n",
      "         8.2657e-01, 2.2604e-01, 7.6778e-01, 7.9905e-01, 8.5525e-01, 3.1713e-01,\n",
      "         1.2560e-01, 8.9682e-01, 4.7970e-01, 4.6706e-02, 7.5905e-01, 5.9773e-02,\n",
      "         9.1201e-01, 2.8403e-01, 4.1517e-02, 5.5958e-01, 5.7390e-01, 5.6910e-01,\n",
      "         6.7476e-01, 4.3764e-01, 5.5570e-01, 9.5368e-01, 4.3681e-01, 2.0087e-02,\n",
      "         1.5264e-01, 6.2356e-01, 9.2074e-01, 4.2888e-01, 3.4788e-01, 1.2440e-01,\n",
      "         7.3454e-01, 3.5302e-01, 4.3647e-01, 5.9630e-01, 3.3836e-01, 8.5962e-01,\n",
      "         1.4381e-01, 4.1810e-01, 7.5695e-01, 6.5548e-01, 3.2322e-01, 5.9725e-03,\n",
      "         3.5092e-01, 4.7582e-01, 5.4596e-01, 1.1383e-01, 3.9919e-01, 2.0570e-02,\n",
      "         9.2494e-01, 8.8966e-01, 2.1126e-01, 1.1916e-01, 4.0832e-01, 4.3147e-01,\n",
      "         2.3043e-01, 9.2147e-01, 9.1767e-01, 3.0912e-01, 3.6043e-01, 7.1918e-01,\n",
      "         8.8883e-01, 1.2205e-02, 2.4875e-01, 1.4896e-01, 8.1487e-01, 7.6726e-01,\n",
      "         1.9141e-01, 7.7394e-01, 8.0613e-01, 3.2433e-01, 5.0031e-01, 6.9259e-01,\n",
      "         7.4544e-01, 1.2143e-01, 3.5291e-02, 6.6372e-01, 6.5903e-01, 5.6142e-01,\n",
      "         5.7377e-01, 6.6026e-01, 7.3383e-01, 8.3993e-01, 8.8201e-01, 6.8030e-01,\n",
      "         7.0639e-01, 5.7643e-01, 6.0210e-01, 9.2726e-01, 9.9973e-01, 1.9205e-01,\n",
      "         5.2911e-01, 9.1228e-01, 5.3825e-01, 8.8398e-01, 5.6659e-01, 8.7674e-01,\n",
      "         3.3199e-01, 1.9243e-01, 2.0361e-01, 4.0150e-01, 6.9597e-01, 6.7411e-01,\n",
      "         7.7396e-01, 1.1062e-01, 4.9855e-01, 5.8805e-02, 7.9740e-01, 8.3476e-01,\n",
      "         3.3252e-01, 3.6910e-01, 9.1980e-01, 5.3011e-01, 4.7515e-01, 9.9121e-01,\n",
      "         8.9961e-01, 2.8230e-01, 2.2313e-01, 6.9143e-01, 3.3328e-01, 8.3813e-01,\n",
      "         9.4306e-01, 8.8144e-01, 1.2317e-01, 4.0560e-01, 8.8871e-01, 1.5789e-01,\n",
      "         9.7753e-01, 6.9339e-01, 6.5762e-01, 8.0210e-01, 3.7810e-01, 6.4026e-01,\n",
      "         1.1515e-01, 9.7164e-01, 9.3559e-01, 2.2297e-02, 8.0779e-01, 5.9200e-01,\n",
      "         5.8872e-01, 4.3577e-01, 4.9127e-01, 1.3204e-01, 1.5908e-01, 1.2819e-01,\n",
      "         7.5135e-01, 6.2607e-01, 9.1940e-01, 4.3569e-01, 3.4176e-01, 7.7599e-01,\n",
      "         2.2909e-01, 5.5489e-01, 9.6184e-04, 7.4150e-01, 9.8699e-01, 5.3297e-01,\n",
      "         2.8051e-01, 8.5647e-02, 9.6840e-01, 2.6976e-01, 4.9454e-01, 5.3375e-01,\n",
      "         9.9240e-01, 6.3652e-01, 1.9903e-01, 6.0397e-01, 3.2525e-01, 7.8025e-01,\n",
      "         5.4218e-01, 3.7242e-01, 2.8414e-01, 6.5593e-01, 9.7067e-01, 7.9905e-01,\n",
      "         3.9911e-01, 5.6035e-01, 6.4570e-02, 9.6225e-01, 7.3593e-01, 1.9287e-01,\n",
      "         9.5238e-01, 2.4343e-01, 2.9010e-02, 4.9936e-01, 7.6322e-02, 8.6277e-01,\n",
      "         2.2460e-01, 2.6954e-01, 9.9032e-01, 3.1569e-01, 3.4838e-01, 3.1296e-01,\n",
      "         1.4506e-01, 6.0873e-01, 8.9725e-01, 1.2847e-01, 6.2057e-01, 8.0230e-01,\n",
      "         7.2684e-01, 1.9933e-01, 9.3159e-01, 5.9827e-01, 4.1086e-01, 4.2625e-02,\n",
      "         6.8494e-01, 5.4315e-01, 7.8917e-01, 6.0542e-01, 6.1342e-01, 9.4217e-01,\n",
      "         5.7510e-01, 3.7523e-01, 5.4445e-01, 3.3241e-01, 2.7996e-01, 1.7718e-02,\n",
      "         5.0552e-01, 5.6808e-01, 1.0143e-01, 4.2360e-01, 2.3628e-01, 1.5366e-01,\n",
      "         8.1442e-01, 8.0890e-01, 8.4375e-02, 4.0137e-01, 6.5711e-01, 4.6212e-01,\n",
      "         1.3342e-01, 4.0787e-02, 8.5901e-01, 8.3637e-01, 5.3429e-01, 6.1109e-01,\n",
      "         5.2062e-01, 4.5335e-01, 3.1814e-01, 5.0893e-01, 1.1073e-01, 9.3669e-01,\n",
      "         4.9412e-01, 8.3852e-01, 5.9187e-01, 3.7491e-01, 4.4184e-01, 8.5390e-01,\n",
      "         4.4388e-01, 7.0481e-01, 4.2110e-01, 3.4486e-01, 6.1155e-01, 9.5214e-01,\n",
      "         2.8763e-01, 2.8286e-01, 4.5253e-01, 4.2236e-01, 4.3413e-02, 8.1216e-01,\n",
      "         2.5045e-01, 2.8462e-01, 5.5599e-01, 8.9761e-01, 9.1063e-01, 7.6681e-01,\n",
      "         7.8934e-01, 8.4998e-01, 6.5848e-01, 2.4396e-01, 9.1543e-02, 4.3418e-01,\n",
      "         5.7594e-01, 3.3511e-01, 8.7146e-01, 5.0858e-01, 6.7434e-01, 6.5846e-01,\n",
      "         4.5468e-01, 7.1708e-01, 6.2626e-01, 4.1356e-01, 2.4017e-01, 4.4758e-01,\n",
      "         1.8181e-01, 4.7410e-02, 3.7395e-01, 9.0842e-01, 8.5876e-01, 5.7282e-01,\n",
      "         3.9084e-01, 1.9172e-02, 3.0530e-01, 6.4542e-01, 4.6885e-01, 9.0498e-01,\n",
      "         2.7966e-01, 5.5610e-01, 6.8001e-01, 4.5122e-01, 2.3000e-01, 3.7034e-01,\n",
      "         2.7969e-01, 7.6292e-01, 3.7996e-01, 4.4045e-02, 6.6524e-01, 6.1221e-01,\n",
      "         1.0073e-01, 2.9232e-01, 1.8038e-01, 9.2379e-02, 5.1530e-01, 3.8492e-01,\n",
      "         5.0874e-01, 2.3715e-01, 1.9476e-01, 3.7980e-01, 7.0113e-01, 6.7698e-01,\n",
      "         4.8324e-01, 5.7315e-02, 2.0574e-01, 9.5456e-01, 5.6917e-01, 3.5656e-01,\n",
      "         8.3093e-01, 2.0753e-01, 4.3313e-01, 3.8482e-01, 8.1088e-01, 5.9434e-01,\n",
      "         2.6953e-01, 4.0523e-01, 2.5909e-02, 6.2401e-01, 6.4627e-01, 1.0452e-01,\n",
      "         3.9467e-03, 5.6883e-03, 9.3371e-01, 5.6279e-01, 9.6664e-01, 2.5998e-01,\n",
      "         6.0369e-01, 9.2919e-01, 4.7627e-01, 8.6400e-01, 8.7280e-01, 5.2345e-01,\n",
      "         6.1853e-02, 5.6140e-01, 9.0019e-01, 6.5069e-01, 5.1123e-01, 5.8876e-01,\n",
      "         6.9650e-01, 9.1344e-01, 7.5589e-01, 6.4261e-01, 2.0024e-02, 8.6294e-01,\n",
      "         6.1039e-01, 8.8997e-01, 8.2676e-01, 3.7294e-01, 3.0113e-01, 2.7875e-01,\n",
      "         9.3143e-01, 8.7158e-01, 4.8306e-01, 9.7959e-01, 6.6534e-01, 2.8018e-01,\n",
      "         9.2090e-01, 1.1369e-01, 1.2331e-01, 6.5872e-01, 7.0002e-01, 8.8824e-01,\n",
      "         8.2043e-01, 4.0840e-01, 9.4350e-02, 8.8645e-02, 6.0935e-01, 4.1746e-01,\n",
      "         6.2461e-01, 2.4788e-01, 8.7791e-01, 6.8499e-01, 8.5677e-01, 1.0835e-01,\n",
      "         7.2809e-01, 9.4738e-01, 8.2508e-01, 5.1675e-01, 8.2895e-01, 1.6171e-02,\n",
      "         7.6814e-01, 7.2859e-01, 8.1605e-01, 7.5943e-01]])\n",
      "model parameter gradients before backwards propagation\n",
      "\n",
      "None\n",
      "\n",
      "model param grads after backprop\n",
      "\n",
      "tensor([ 1.9560e-03, -1.5917e-03,  0.0000e+00, -1.9051e-03,  0.0000e+00,\n",
      "        -3.3632e-03, -2.7124e-03,  0.0000e+00,  3.3807e-03,  0.0000e+00,\n",
      "        -5.7307e-03,  4.2510e-03,  1.5018e-03,  0.0000e+00,  3.1916e-03,\n",
      "         3.2398e-03, -8.4237e-04,  3.1686e-03,  2.3321e-03, -9.7898e-03,\n",
      "         3.8244e-03,  4.2180e-03, -3.8839e-03,  8.8370e-04, -1.8048e-03,\n",
      "        -3.4274e-03,  2.0489e-03,  5.7230e-03,  8.5663e-04, -2.4579e-04,\n",
      "         6.8061e-04,  1.2951e-03, -1.4022e-03,  1.7539e-03, -9.1372e-04,\n",
      "         2.3550e-03,  0.0000e+00, -3.5197e-03,  0.0000e+00,  6.0130e-03,\n",
      "        -5.7013e-03,  7.3333e-03, -2.5536e-03, -3.2186e-03, -1.2166e-03,\n",
      "        -7.2683e-04,  2.4582e-03,  3.2178e-03,  0.0000e+00,  1.5843e-03,\n",
      "        -2.6206e-03, -6.5104e-05,  1.5006e-03,  1.5752e-03,  4.1911e-04,\n",
      "        -4.0584e-03,  1.7449e-04, -6.6451e-03,  9.0608e-04,  9.2045e-03,\n",
      "        -1.0538e-03, -1.0115e-03, -1.3173e-03,  8.1092e-03])\n",
      "\n",
      "model parameter values before gradient descent propagation\n",
      "\n",
      "tensor([ 2.3487e-01,  2.6626e-01, -5.1096e-08,  5.1870e-01,  3.4404e-09,\n",
      "         2.2239e-01,  4.2289e-01,  1.3153e-07,  2.5093e-01,  1.5152e-06,\n",
      "         3.1687e-01,  2.5049e-01,  3.7893e-01,  1.0862e-05,  2.7526e-01,\n",
      "         2.3674e-01,  2.4202e-01,  3.9531e-01,  4.6935e-01,  2.9090e-01,\n",
      "         2.7268e-01,  2.7803e-01,  2.9069e-01,  2.0693e-01,  2.5899e-01,\n",
      "         2.7871e-01,  2.9115e-01,  3.1601e-01,  3.8889e-01,  3.0411e-01,\n",
      "         2.6776e-01,  2.1093e-01,  2.8708e-01,  3.3243e-01,  4.2673e-01,\n",
      "         3.7326e-01,  7.4804e-08,  1.9068e-01,  1.4740e-08,  2.2303e-01,\n",
      "         1.7908e-01,  2.4860e-01,  2.7400e-01,  2.5923e-01,  2.9420e-01,\n",
      "         2.9924e-01,  2.2369e-01,  2.6280e-01,  2.2001e-08,  2.6610e-01,\n",
      "         2.2089e-01,  2.8429e-01,  3.3072e-01,  2.2681e-01,  3.6538e-01,\n",
      "         2.1230e-01,  2.3965e-01,  2.4950e-01,  5.2583e-01,  2.4825e-01,\n",
      "         2.9565e-01,  2.5878e-01,  4.8326e-01,  2.6670e-01])\n",
      "\n",
      "model parameter values after gradient descent propagation\n",
      "\n",
      "tensor([ 2.3485e-01,  2.6627e-01, -5.1096e-08,  5.1872e-01,  3.4404e-09,\n",
      "         2.2242e-01,  4.2291e-01,  1.3153e-07,  2.5090e-01,  1.5152e-06,\n",
      "         3.1693e-01,  2.5045e-01,  3.7891e-01,  1.0862e-05,  2.7523e-01,\n",
      "         2.3671e-01,  2.4203e-01,  3.9528e-01,  4.6932e-01,  2.9099e-01,\n",
      "         2.7265e-01,  2.7799e-01,  2.9073e-01,  2.0692e-01,  2.5901e-01,\n",
      "         2.7874e-01,  2.9113e-01,  3.1596e-01,  3.8888e-01,  3.0411e-01,\n",
      "         2.6775e-01,  2.1091e-01,  2.8710e-01,  3.3241e-01,  4.2674e-01,\n",
      "         3.7324e-01,  7.4804e-08,  1.9071e-01,  1.4740e-08,  2.2297e-01,\n",
      "         1.7914e-01,  2.4853e-01,  2.7402e-01,  2.5926e-01,  2.9421e-01,\n",
      "         2.9924e-01,  2.2366e-01,  2.6277e-01,  2.2001e-08,  2.6608e-01,\n",
      "         2.2092e-01,  2.8429e-01,  3.3071e-01,  2.2679e-01,  3.6538e-01,\n",
      "         2.1234e-01,  2.3965e-01,  2.4956e-01,  5.2582e-01,  2.4816e-01,\n",
      "         2.9566e-01,  2.5879e-01,  4.8327e-01,  2.6662e-01])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Hoe torch.autograd werkt\n",
    "'''\n",
    "\n",
    "import torch\n",
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "model = resnet18(weights=ResNet18_Weights.DEFAULT)\n",
    "data = torch.rand(1, 3, 64, 64)\n",
    "labels = torch.rand(1, 1000)\n",
    "print(labels)\n",
    "\n",
    "print(f\"model parameter gradients before backwards propagation\\n\")\n",
    "for idx, param in enumerate(model.parameters()):\n",
    "    if idx == 1:\n",
    "        print(f\"{param.grad}\\n\")\n",
    "\n",
    "prediction = model(data)\n",
    "loss = (prediction - labels).sum()\n",
    "loss.backward()\n",
    "\n",
    "print(f\"model param grads after backprop\\n\")\n",
    "for idx, param in enumerate(model.parameters()):\n",
    "    if idx == 1:\n",
    "        print(f\"{param.grad}\\n\")\n",
    "\n",
    "print(f\"model parameter values before gradient descent propagation\\n\")\n",
    "for idx, param in enumerate(model.parameters()):\n",
    "    if idx == 1:\n",
    "        print(f\"{param.data}\\n\")\n",
    "\n",
    "optim = torch.optim.SGD(model.parameters(), lr=1e-2, momentum=0.9)\n",
    "optim.step()\n",
    "\n",
    "print(f\"model parameter values after gradient descent propagation\\n\")\n",
    "for idx, param in enumerate(model.parameters()):\n",
    "    if idx == 1:\n",
    "        print(f\"{param.data}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "819863e0-4866-46d3-a11f-218d981d9fed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-12.,  65.], grad_fn=<SubBackward0>)\n",
      "tensor([True, True])\n",
      "tensor([True, True])\n",
      "tensor([-12.,  65.], grad_fn=<SubBackward0>)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Explanation of differentiation in autograd\n",
    "'''\n",
    "\n",
    "import torch\n",
    "\n",
    "a = torch.tensor([2., 3.], requires_grad=True)\n",
    "b = torch.tensor([6., 4.], requires_grad=True)\n",
    "\n",
    "Q = 3*a**3 - b**2\n",
    "print(Q)\n",
    "\n",
    "external_grad = torch.tensor([1., 1.])\n",
    "Q.backward(gradient=external_grad)\n",
    "\n",
    "# check if collected gradients are correct\n",
    "print(9*a**2 == a.grad)\n",
    "print(-2*b == b.grad)\n",
    "print(Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2170764c-2819-4d7c-8a9e-17b0f8b1c950",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "How to freeze (part of) a CNN\n",
    "'''\n",
    "\n",
    "from torch import nn, optim\n",
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "\n",
    "model = resnet18(weights=ResNet18_Weights.DEFAULT)\n",
    "\n",
    "# for layer in model.named_parameters():\n",
    "#     print(layer)\n",
    "\n",
    "# Freeze all the parameters in the network\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "model.fc = nn.Linear(512, 10)\n",
    "\n",
    "# Optimize only the classifier\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-2, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45bb5262-6fad-4e9f-968b-169a1ae31652",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=400, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # 1 input image channel, 6 output channels, 5x5 square convolution\n",
    "        # kernel\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        # an affine operation: y = Wx + b\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)  # 5*5 from image dimension\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, input):\n",
    "        # Convolution layer C1: 1 input image channel, 6 output channels,\n",
    "        # 5x5 square convolution, it uses RELU activation function, and\n",
    "        # outputs a Tensor with size (N, 6, 28, 28), where N is the size of the batch\n",
    "        c1 = F.relu(self.conv1(input))\n",
    "        # Subsampling layer S2: 2x2 grid, purely functional,\n",
    "        # this layer does not have any parameter, and outputs a (N, 6, 14, 14) Tensor\n",
    "        s2 = F.max_pool2d(c1, (2, 2))\n",
    "        # Convolution layer C3: 6 input channels, 16 output channels,\n",
    "        # 5x5 square convolution, it uses RELU activation function, and\n",
    "        # outputs a (N, 16, 10, 10) Tensor\n",
    "        c3 = F.relu(self.conv2(s2))\n",
    "        # Subsampling layer S4: 2x2 grid, purely functional,\n",
    "        # this layer does not have any parameter, and outputs a (N, 16, 5, 5) Tensor\n",
    "        s4 = F.max_pool2d(c3, 2)\n",
    "        # Flatten operation: purely functional, outputs a (N, 400) Tensor\n",
    "        s4 = torch.flatten(s4, 1)\n",
    "        # Fully connected layer F5: (N, 400) Tensor input,\n",
    "        # and outputs a (N, 120) Tensor, it uses RELU activation function\n",
    "        f5 = F.relu(self.fc1(s4))\n",
    "        # Fully connected layer F6: (N, 120) Tensor input,\n",
    "        # and outputs a (N, 84) Tensor, it uses RELU activation function\n",
    "        f6 = F.relu(self.fc2(f5))\n",
    "        # Gaussian layer OUTPUT: (N, 84) Tensor input, and\n",
    "        # outputs a (N, 10) Tensor\n",
    "        output = self.fc3(f6)\n",
    "        return output\n",
    "\n",
    "\n",
    "net = Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ba7c0493-35fc-4155-b0ab-0c46ebc53720",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "torch.Size([6, 1, 5, 5])\n"
     ]
    }
   ],
   "source": [
    "params = list(net.parameters())\n",
    "print(len(params))\n",
    "print(params[0].size())  # conv1's .weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c2935b53-7732-465c-9725-828e2d71ae1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[37., 36., 35.,  ..., 51., 52., 52.],\n",
      "         [38., 37., 37.,  ..., 51., 51., 52.],\n",
      "         [39., 39., 38.,  ..., 52., 51., 51.],\n",
      "         ...,\n",
      "         [89., 89., 89.,  ..., 26., 30., 32.],\n",
      "         [88., 88., 88.,  ..., 31., 35., 37.],\n",
      "         [87., 88., 88.,  ..., 33., 37., 39.]],\n",
      "\n",
      "        [[47., 46., 44.,  ..., 59., 60., 60.],\n",
      "         [48., 47., 46.,  ..., 59., 59., 60.],\n",
      "         [49., 49., 48.,  ..., 60., 59., 59.],\n",
      "         ...,\n",
      "         [84., 84., 84.,  ..., 27., 29., 31.],\n",
      "         [83., 83., 83.,  ..., 32., 34., 36.],\n",
      "         [82., 83., 83.,  ..., 34., 36., 38.]],\n",
      "\n",
      "        [[56., 55., 53.,  ..., 61., 62., 62.],\n",
      "         [57., 56., 55.,  ..., 61., 61., 62.],\n",
      "         [58., 58., 57.,  ..., 62., 61., 61.],\n",
      "         ...,\n",
      "         [54., 54., 54.,  ..., 32., 37., 39.],\n",
      "         [53., 53., 53.,  ..., 37., 40., 42.],\n",
      "         [52., 53., 53.,  ..., 39., 42., 44.]]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "from torchvision import transforms as transform\n",
    "\n",
    "im = Image.open('./eagle.jpg')\n",
    "imTensor = transform.functional.pil_to_tensor(im)\n",
    "imTensor = imTensor.float()\n",
    "imTensor = imTensor.requires_grad_(True)\n",
    "print(imTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9d5254ba-ecef-4da4-8099-b81858e4e49f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0913,  0.0627,  0.0170, -0.0013, -0.0475, -0.0181,  0.0994, -0.0417,\n",
      "         -0.0979, -0.1149]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "input = torch.randn(1, 1, 32, 32)\n",
    "out = net(input)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c0d8d6df-4012-4a1d-b77b-5c5703554362",
   "metadata": {},
   "outputs": [],
   "source": [
    "net.zero_grad()\n",
    "out.backward(torch.randn(1, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4e6a7867-f201-4bcd-86db-328e8c7d9d5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.0830, grad_fn=<MseLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "output = net(input)\n",
    "target = torch.randn(10)  # a dummy target, for example\n",
    "target = target.view(1, -1)  # make it the same shape as output\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "loss = criterion(output, target)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "579405f7-3619-49cc-bcb9-8b3dbbd5474a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<MseLossBackward0 object at 0x73f1a5bc63b0>\n",
      "<AddmmBackward0 object at 0x73f1a5bc4b50>\n",
      "<AccumulateGrad object at 0x73f1a5bc4b50>\n"
     ]
    }
   ],
   "source": [
    "print(loss.grad_fn)  # MSELoss\n",
    "print(loss.grad_fn.next_functions[0][0])  # Linear\n",
    "print(loss.grad_fn.next_functions[0][0].next_functions[0][0])  # ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4ed1bf0b-5a7a-4967-869f-66df660d7bee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.bias.grad before backward\n",
      "None\n",
      "conv1.bias.grad after backward\n",
      "tensor([-0.0094,  0.0021, -0.0162, -0.0068, -0.0159, -0.0153])\n"
     ]
    }
   ],
   "source": [
    "net.zero_grad()     # zeroes the gradient buffers of all parameters\n",
    "\n",
    "print('conv1.bias.grad before backward')\n",
    "print(net.conv1.bias.grad)\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "print('conv1.bias.grad after backward')\n",
    "print(net.conv1.bias.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3a232272-2cb9-42b3-bf94-653486339ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "for f in net.parameters():\n",
    "    f.data.sub_(f.grad.data * learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "45d779da-d254-4ba9-b61a-4df5466486a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# create your optimizer\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.01)\n",
    "\n",
    "# in your training loop:\n",
    "optimizer.zero_grad()   # zero the gradient buffers\n",
    "output = net(input)\n",
    "loss = criterion(output, target)\n",
    "loss.backward()\n",
    "optimizer.step()    # Does the update"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
